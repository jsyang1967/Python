{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/05/cd/c171d2e33c0192b04560ce864c26eba83fed888fe5cd9ded661b2702f2ae/tensorflow-1.12.0-cp36-cp36m-win_amd64.whl (45.9MB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting tensorboard<1.13.0,>=1.12.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/e0/d0/65fe48383146199f16dbd5999ef226b87bce63ad5cd73c840cf722637969/tensorboard-1.12.0-py3-none-any.whl (3.0MB)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/0c/63/f505d2d4c21db849cf80bad517f0065a30be6b006b0a5637f1b95584a305/absl-py-0.6.1.tar.gz (94kB)\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/94/74e0fa783d3fc07e41715973435dd051ca89c550881b3454233c39c73e69/Keras_Preprocessing-1.0.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.31.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.0)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/e8/df/d606d07cff0fc8d22abcc54006c0247002d11a7f2d218eb008d48e76851d/protobuf-3.6.1-cp36-cp36m-win_amd64.whl (1.1MB)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/90/520757ccafb14f03e8e46a54bacd45f5f9cca6b96b58b83b66a272f059df/grpcio-1.16.0-cp36-cp36m-win_amd64.whl (1.5MB)\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/3f/c4/2ff40221029f7098d58f8d7fb99b97e8100f3293f9856f0fb5834bef100b/Keras_Applications-1.0.6-py2.py3-none-any.whl (44kB)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/78/ff794fcae2ce8aa6323e789d1f8b3b7765f601e7702726f430e814822b96/gast-0.2.0.tar.gz\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.13.0,>=1.12.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl (89kB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (39.1.0)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.7.1)\n",
      "Building wheels for collected packages: termcolor, absl-py, gast\n",
      "  Running setup.py bdist_wheel for termcolor: started\n",
      "  Running setup.py bdist_wheel for termcolor: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for absl-py: started\n",
      "  Running setup.py bdist_wheel for absl-py: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\18\\ea\\5e\\e36e1b8739e78cd2eba0a08fdc602c2b16a4b263912af8cb64\n",
      "  Running setup.py bdist_wheel for gast: started\n",
      "  Running setup.py bdist_wheel for gast: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\pip\\Cache\\wheels\\9a\\1f\\0e\\3cde98113222b853e98fc0a8e9924480a3e25f1b4008cedb4f\n",
      "Successfully built termcolor absl-py gast\n",
      "Installing collected packages: termcolor, protobuf, grpcio, markdown, tensorboard, astor, absl-py, keras-preprocessing, keras-applications, gast, tensorflow\n",
      "Successfully installed absl-py-0.6.1 astor-0.7.1 gast-0.2.0 grpcio-1.16.0 keras-applications-1.0.6 keras-preprocessing-1.0.5 markdown-3.0.1 protobuf-3.6.1 tensorboard-1.12.0 tensorflow-1.12.0 termcolor-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-a52087e6ef24>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting C:\\Users\\USER\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting C:\\Users\\USER\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting C:\\Users\\USER\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting C:\\Users\\USER\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"C:\\\\Users\\\\USER\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Datasets(train=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000000010F9B400>, validation=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x00000000120C1710>, test=<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet object at 0x0000000006A02EB8>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '_asdict', '_fields', '_make', '_replace', '_source', 'count', 'index', 'test', 'train', 'validation']\n"
     ]
    }
   ],
   "source": [
    "print(dir(mnist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_epochs_completed', '_images', '_index_in_epoch', '_labels', '_num_examples', 'epochs_completed', 'images', 'labels', 'next_batch', 'num_examples']\n"
     ]
    }
   ],
   "source": [
    "print(dir(mnist.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = mnist.train.images[0].reshape((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12bcf588>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 256 \n",
    "n_input    = 784 \n",
    "n_classes  = 10 \n",
    "n_samples  = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):  \n",
    "    # First Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # Second Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Last Output layer\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsamp,ysamp = mnist.train.next_batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xsamp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ysamp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsamp,ysamp = mnist.train.next_batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1225cf60>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_image = Xsamp.reshape((28,28))\n",
    "plt.imshow(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ysamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_2:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Equal:0' shape=(?,) dtype=bool>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = tf.cast(correct_predictions, \"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost=147.24263364618452\n",
      "Epoch: 2 cost=57.525037791512304\n",
      "Epoch: 3 cost=42.882276125821186\n",
      "Epoch: 4 cost=35.18514755422422\n",
      "Epoch: 5 cost=29.938363587206094\n",
      "Epoch: 6 cost=26.270296014222247\n",
      "Epoch: 7 cost=23.28003679969093\n",
      "Epoch: 8 cost=21.118173128474858\n",
      "Epoch: 9 cost=19.20924246267838\n",
      "Epoch: 10 cost=17.562148612629287\n",
      "Epoch: 11 cost=16.2069444407845\n",
      "Epoch: 12 cost=14.997819293087163\n",
      "Epoch: 13 cost=14.059020807851429\n",
      "Epoch: 14 cost=13.093359111330729\n",
      "Epoch: 15 cost=12.305078910372481\n",
      "0.908\n",
      "Training Completed in 15 Epochs\n"
     ]
    }
   ],
   "source": [
    "# Start the session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Intialize all the variables\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # Initial the cost = 0.0\n",
    "        avg_cost = 0.0\n",
    "\n",
    "        total_batch = int(n_samples/batch_size)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print(\"Epoch: {} cost={}\".format(epoch+1,avg_cost))\n",
    "    print(accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "\n",
    "print(\"Training Completed in {} Epochs\".format(training_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\USER\\train-images-idx3-ubyte.gz\n",
      "Extracting C:\\Users\\USER\\train-labels-idx1-ubyte.gz\n",
      "Extracting C:\\Users\\USER\\t10k-images-idx3-ubyte.gz\n",
      "Extracting C:\\Users\\USER\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"C:\\\\Users\\\\USER\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.001\n",
    "training_epochs = 15\n",
    "batch_size      = 100\n",
    "\n",
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 256 \n",
    "n_input    = 784 \n",
    "n_classes  = 10 \n",
    "n_samples  = mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('inputs'):\n",
    "    x = tf.placeholder(\"float\", [None, n_input],   name= 'input_x')\n",
    "    y = tf.placeholder(\"float\", [None, n_classes], name= 'input_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('input_reshape'):\n",
    "    image_input = tf.reshape(x,[-1,28,28,1])\n",
    "    tf.summary.image('input', image_input, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer(x, input_tensors, output_tensors, layer_name, activation_function = None):  \n",
    "    with tf.name_scope('Layer'):\n",
    "        with tf.name_scope('Weights'):\n",
    "            weight = tf.Variable(tf.random_normal([input_tensors, output_tensors]), name = 'w')\n",
    "            tf.summary.histogram(name = layer_name + '/Weights', values = weight)\n",
    "        with tf.name_scope('Bias'):\n",
    "            bias = tf.Variable(tf.random_normal([output_tensors]), name= 'b')\n",
    "            tf.summary.histogram(name = layer_name + '/Bias', values = bias)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            formula = tf.add(tf.matmul(x, weight), bias)\n",
    "        if activation_function is None:\n",
    "            outputs = formula\n",
    "        else:\n",
    "            outputs = activation_function(formula)\n",
    "        tf.summary.histogram(name = layer_name + '/Outputs', values = outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = add_layer(x, input_tensors = n_input, output_tensors = n_hidden_1, layer_name='layer1',activation_function = tf.nn.relu)\n",
    "layer2 = add_layer(layer1, input_tensors = n_hidden_1, output_tensors = n_hidden_2, layer_name='layer2',activation_function = tf.nn.relu)\n",
    "out_layer = add_layer(layer2, input_tensors = n_hidden_2, output_tensors = n_classes, layer_name='out_layer',activation_function = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-39-85aab1677ab7>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('cost'):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_layer, labels=y))\n",
    "    tf.summary.scalar('loss', cost)\n",
    "    \n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "with tf.name_scope('Accuracy'):\n",
    "    acc = tf.equal(tf.argmax(out_layer, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost=147.96889529488297\n",
      "Epoch: 2 cost=54.86905582427978\n",
      "Epoch: 3 cost=40.9968935931813\n",
      "Epoch: 4 cost=33.76505600148982\n",
      "Epoch: 5 cost=28.850167244997888\n",
      "Epoch: 6 cost=25.281023020094096\n",
      "Epoch: 7 cost=22.58463800053706\n",
      "Epoch: 8 cost=20.41862362212051\n",
      "Epoch: 9 cost=18.55831642584367\n",
      "Epoch: 10 cost=17.10417472221635\n",
      "Epoch: 11 cost=15.792815682021038\n",
      "Epoch: 12 cost=14.67900391405278\n",
      "Epoch: 13 cost=13.73546918002041\n",
      "Epoch: 14 cost=12.861662544553923\n",
      "Epoch: 15 cost=12.07129249435933\n",
      "Training Completed in 15 Epochs\n"
     ]
    }
   ],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    ## Merge Summary\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"tensorboard2/\", graph = sess.graph)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(n_samples/batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            _, c, result = sess.run([optimizer, cost, merged], feed_dict={x: batch_x, y: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "            ## Adding summary of each step\n",
    "            writer.add_summary(result,  epoch * total_batch + i)\n",
    "\n",
    "        print(\"Epoch: {} cost={}\".format(epoch+1,avg_cost))\n",
    "\n",
    "    print(\"Training Completed in {} Epochs\".format(training_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.14.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (2.7.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.0.5)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (3.12)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.0.6)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.11.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Activation, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 15s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x140d15c0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test  = x_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test  /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes  = 10 \n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 256 \n",
    "n_input    = 784 \n",
    "n_classes  = 10 \n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Dense(n_hidden_1, activation='relu', input_shape=(n_input,)))\n",
    "model.add(Dense(n_hidden_2, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 1.0676 - acc: 0.7464 - val_loss: 0.4884 - val_acc: 0.8752\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.4197 - acc: 0.8884 - val_loss: 0.3514 - val_acc: 0.9043\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.3403 - acc: 0.9054 - val_loss: 0.3063 - val_acc: 0.9170\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.3030 - acc: 0.9143 - val_loss: 0.2819 - val_acc: 0.9218\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2784 - acc: 0.9213 - val_loss: 0.2596 - val_acc: 0.9279\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.2592 - acc: 0.9268 - val_loss: 0.2470 - val_acc: 0.9302\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.2431 - acc: 0.9309 - val_loss: 0.2328 - val_acc: 0.9353\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.2291 - acc: 0.9351 - val_loss: 0.2203 - val_acc: 0.9387\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.2170 - acc: 0.9392 - val_loss: 0.2110 - val_acc: 0.9422\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.2059 - acc: 0.9418 - val_loss: 0.2006 - val_acc: 0.9429\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1962 - acc: 0.9445 - val_loss: 0.1928 - val_acc: 0.9453\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1872 - acc: 0.9472 - val_loss: 0.1850 - val_acc: 0.9456\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1791 - acc: 0.9489 - val_loss: 0.1795 - val_acc: 0.9475\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1715 - acc: 0.9512 - val_loss: 0.1714 - val_acc: 0.9505\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1645 - acc: 0.9529 - val_loss: 0.1643 - val_acc: 0.9519\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=training_epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.argmax(model.predict(x_test), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  PIL import Image\n",
    "img = Image.open('C:\\\\Users\\\\user\\\\bw.jpg').convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'> 12 255\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(type(img), np.min(img), np.max(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 400)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.array(img) \n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[254, 254, 254, ..., 254, 254, 254],\n",
       "       [254, 254, 254, ..., 254, 254, 254],\n",
       "       [254, 254, 254, ..., 254, 254, 254],\n",
       "       ...,\n",
       "       [254, 254, 254, ..., 254, 254, 254],\n",
       "       [254, 254, 254, ..., 254, 254, 254],\n",
       "       [254, 254, 254, ..., 254, 254, 254]], dtype=uint8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  PIL import Image\n",
    "img = Image.open('bw.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Activation, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test  = x_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test  /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes  = 10 \n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = 256 \n",
    "n_hidden_2 = 256 \n",
    "n_input    = 784 \n",
    "n_classes  = 10 \n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Dense(n_hidden_1, activation='relu', input_shape=(n_input,)))\n",
    "model.add(Dense(n_hidden_2, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 4s 59us/step - loss: 0.2442 - acc: 0.9296 - val_loss: 0.1163 - val_acc: 0.9643\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0932 - acc: 0.9714 - val_loss: 0.0930 - val_acc: 0.9708\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 3s 52us/step - loss: 0.0607 - acc: 0.9807 - val_loss: 0.0797 - val_acc: 0.9739\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 3s 58us/step - loss: 0.0437 - acc: 0.9857 - val_loss: 0.0806 - val_acc: 0.9745\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0310 - acc: 0.9907 - val_loss: 0.0728 - val_acc: 0.9782\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 3s 51us/step - loss: 0.0246 - acc: 0.9921 - val_loss: 0.0815 - val_acc: 0.9746\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 3s 48us/step - loss: 0.0221 - acc: 0.9925 - val_loss: 0.0737 - val_acc: 0.9786\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 3s 49us/step - loss: 0.0163 - acc: 0.9948 - val_loss: 0.0687 - val_acc: 0.9821\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.0154 - acc: 0.9946 - val_loss: 0.0889 - val_acc: 0.9775\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0177 - acc: 0.9941 - val_loss: 0.0889 - val_acc: 0.9784\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0108 - acc: 0.9966 - val_loss: 0.0773 - val_acc: 0.9806\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 3s 55us/step - loss: 0.0127 - acc: 0.9959 - val_loss: 0.0811 - val_acc: 0.9808\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0833 - val_acc: 0.9813\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 3s 54us/step - loss: 0.0082 - acc: 0.9975 - val_loss: 0.1034 - val_acc: 0.9764\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 3s 53us/step - loss: 0.0134 - acc: 0.9956 - val_loss: 0.0875 - val_acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=training_epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "res = requests.get('https://www.google.com.tw/search?ei=lqfnW4axMZLI8wX1o7T4Cw&yv=3&q={}&tbm=isch&vet=10ahUKEwiG1NOnuMveAhUS5LwKHfURDb8QuT0IgQEoAQ.lqfnW4axMZLI8wX1o7T4Cw.i&ved=0ahUKEwiG1NOnuMveAhUS5LwKHfURDb8QuT0IgQEoAQ&ijn=2&start={}&asearch=ichunk&async=_id:rg_s,_pms:s,_fmt:pc'.format('周子瑜', 200))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###使用CNN 辨識手寫數字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##資料前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 28,28,1)\n",
    "x_test = x_test.reshape(10000, 28,28,1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test  /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test  = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_1 = 256 \n",
    "n_classes  = 10 \n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size = (3,3),\n",
    "            padding = 'same',\n",
    "            input_shape = (28,28,1), \n",
    "            activation = 'relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(n_hidden_1, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 27s 451us/step - loss: 0.2116 - acc: 0.9384 - val_loss: 0.0923 - val_acc: 0.9717\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 25s 425us/step - loss: 0.0687 - acc: 0.9792 - val_loss: 0.0682 - val_acc: 0.9782\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 26s 426us/step - loss: 0.0447 - acc: 0.9860 - val_loss: 0.0535 - val_acc: 0.9820\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 26s 428us/step - loss: 0.0315 - acc: 0.9905 - val_loss: 0.0432 - val_acc: 0.9854\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 26s 426us/step - loss: 0.0229 - acc: 0.9931 - val_loss: 0.0428 - val_acc: 0.9860\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 26s 429us/step - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0562 - val_acc: 0.9827\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 25s 422us/step - loss: 0.0126 - acc: 0.9963 - val_loss: 0.0514 - val_acc: 0.9829\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 26s 430us/step - loss: 0.0102 - acc: 0.9972 - val_loss: 0.0515 - val_acc: 0.9847\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 25s 423us/step - loss: 0.0087 - acc: 0.9972 - val_loss: 0.0433 - val_acc: 0.9874\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 25s 423us/step - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0453 - val_acc: 0.9877\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 25s 425us/step - loss: 0.0056 - acc: 0.9984 - val_loss: 0.0519 - val_acc: 0.9857\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 25s 425us/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0523 - val_acc: 0.9850\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 25s 425us/step - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0523 - val_acc: 0.9871\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 25s 424us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0527 - val_acc: 0.9867\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 26s 427us/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.0517 - val_acc: 0.9876\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=15,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'idol1/'\n",
    "for link in soup.select('img'):\n",
    "    res = requests.get(link.get('src'))\n",
    "    fname = path + link.get('src').split('tbn:')[1] +'.jpg'\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "for n in range(0,3):\n",
    "    res = requests.get('https://www.google.com.tw/search?ei=lqfnW4axMZLI8wX1o7T4Cw&yv=3&q={}&tbm=isch&vet=10ahUKEwiG1NOnuMveAhUS5LwKHfURDb8QuT0IgQEoAQ.lqfnW4axMZLI8wX1o7T4Cw.i&ved=0ahUKEwiG1NOnuMveAhUS5LwKHfURDb8QuT0IgQEoAQ&ijn=2&start={}&asearch=ichunk&async=_id:rg_s,_pms:s,_fmt:pc'.format('周子瑜', n * 100))\n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    path = 'idol1/'\n",
    "    for link in soup.select('img'):\n",
    "        res = requests.get(link.get('src'))\n",
    "        fname = path + link.get('src').split('tbn:')[1] +'.jpg'\n",
    "        with open(fname, 'wb') as f:\n",
    "            f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def getIdolPicture(path, idol):\n",
    "    for n in range(0,3):\n",
    "        res = requests.get('https://www.google.com.tw/search?ei=lqfnW4axMZLI8wX1o7T4Cw&yv=3&q={}&tbm=isch&vet=10ahUKEwiG1NOnuMveAhUS5LwKHfURDb8QuT0IgQEoAQ.lqfnW4axMZLI8wX1o7T4Cw.i&ved=0ahUKEwiG1NOnuMveAhUS5LwKHfURDb8QuT0IgQEoAQ&ijn=2&start={}&asearch=ichunk&async=_id:rg_s,_pms:s,_fmt:pc'.format(idol, n * 100))\n",
    "        soup = BeautifulSoup(res.text, 'lxml')\n",
    "        #path = 'idol1/'\n",
    "        for link in soup.select('img'):\n",
    "            res = requests.get(link.get('src'))\n",
    "            fname = path + link.get('src').split('tbn:')[1] +'.jpg'\n",
    "            with open(fname, 'wb') as f:\n",
    "                f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "getIdolPicture('idol2/', '郭雪芙')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "getIdolPicture('idol3/', '蔡依林')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
